架构题 1:
一套业务系统, 由数据库(mysql, redis 和 postgres), api 网关服务, 各类处理业务的微服务, 前端组成

任务一:
请基于容器设计一套业务系统


任务二:
不采用容器和虚拟机, 基于 pacemaker+corosync 和 haproxy 等组件设计一套业务系统


要求:
1.整个系统无单点故障, 出现故障在秒内完成切换
2.系统的所有业务做到负载均衡, 业务压测时压力平均分配
3.整个系统待机,硬件故障和任何一个服务出现问题, 系统不受影响
4.整个系统资源消耗尽可能少
5.系统安全尽可能的考虑

架构题 2:
三节点的 k8s, master 和 worker 共享三节点(3 master + 3 worker), 当把其中一台(也正好是 etcd master 节点)直接断电后, k8s 需要一段时间来恢复工作, 这个收敛时间如何分布? 如何优化收敛时间?
检查收敛时间:
kubectl 命令恢复工作的收敛时间, e.g 通过观察 kubectl get node
service 接通 new pod (在断电时 schedule 到其他节点的 new pod)的收敛时间, 通过持续 curl url 观察.

3节点信息:
hostname: ip
master0   10.0.0.1
master1   10.0.0.2
master2   10.0.0.3

3 节点etcd-cluster, 安装步骤参看[install guide](https://jimmysong.io/kubernetes-handbook/practice/etcd-cluster-installation.html)

heartbeat interval: 心跳间隔, 默认为 100ms == 0.5-1.5 RTT
election timeout RTT(round-trip time): 选举超时, 默认为 1000ms = 10 RTT, 最大为 50000ms


1. master0:
```
/opt/bin/etcd --name etcd0 --data-dir /var/etcd/data --listen-client-urls https://10.0.0.1:2379,https://127.0.0.1:2379 \
              --listen-peer-urls https://10.0.0.1:2380 --advertise-client-urls https://10.0.0.1:4001 \
              --initial-advertise-peer-urls https://10.0.0.1:2380 --initial-cluster-token etcd-cluster-1 \
              --initial-cluster etcd0=https://10.0.0.1:2380,etcd1=https://10.0.0.2:2380,etcd2=https://10.0.0.3:2380 \
              --initial-cluster-state existing --auto-compaction-retention=1 --client-cert-auth \
              --trusted-ca-file=/etc/kubernetes/cert/ca.pem --cert-file=/etc/kubernetes/cert/etcd.pem \
              --key-file=/etc/kubernetes/cert/etcd-key.pem --peer-client-cert-auth \
              --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem --peer-cert-file=/etc/kubernetes/cert/etcd.pem \
              --peer-key-file=/etc/kubernetes/cert/etcd-key.pem --quota-backend-bytes=8589934592 = $((8*1024*1024*1024))
```

2. master1:
```
/opt/bin/etcd --name etcd1 --data-dir /var/etcd/data --listen-client-urls https://10.0.0.2:2379,https://127.0.0.1:2379 \
              --listen-peer-urls https://10.0.0.2:2380 --advertise-client-urls https://10.0.0.2:4001 \
              --initial-advertise-peer-urls https://10.0.0.2:2380 --initial-cluster-token etcd-cluster-1 \
              --initial-cluster etcd0=https://10.0.0.1:2380,etcd1=https://10.0.0.2:2380,etcd2=https://10.0.0.3:2380 \
              --initial-cluster-state existing --auto-compaction-retention=1 --client-cert-auth \
              --trusted-ca-file=/etc/kubernetes/cert/ca.pem --cert-file=/etc/kubernetes/cert/etcd.pem \
              --key-file=/etc/kubernetes/cert/etcd-key.pem --peer-client-cert-auth \
              --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem --peer-cert-file=/etc/kubernetes/cert/etcd.pem \
              --peer-key-file=/etc/kubernetes/cert/etcd-key.pem --quota-backend-bytes=8589934592
```

3. master2:
```
/opt/bin/etcd --name etcd2 --data-dir /var/etcd/data --listen-client-urls https://10.0.0.3:2379,https://127.0.0.1:2379 \
              --listen-peer-urls https://10.0.0.3:2380 --advertise-client-urls https://10.0.0.3:4001 \
              --initial-advertise-peer-urls https://10.0.0.3:2380 --initial-cluster-token etcd-cluster-1 \
              --initial-cluster etcd0=https://10.0.0.1:2380,etcd1=https://10.0.0.2:2380,etcd2=https://10.0.0.3:2380 \
              --initial-cluster-state existing --auto-compaction-retention=1 --client-cert-auth \
              --trusted-ca-file=/etc/kubernetes/cert/ca.pem --cert-file=/etc/kubernetes/cert/etcd.pem \
              --key-file=/etc/kubernetes/cert/etcd-key.pem --peer-client-cert-auth \
              --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem --peer-cert-file=/etc/kubernetes/cert/etcd.pem \
              --peer-key-file=/etc/kubernetes/cert/etcd-key.pem --quota-backend-bytes=8589934592
```

check etcd status:
```
etcdctl --endpoints=https://10.0.0.1:2379 --cert-file=/etc/kubernetes/cert/etcd.pem \
        --key-file=/etc/kubernetes/cert/etcd-key.pem --ca-file=/etc/kubernetes/cert/ca.pem member list
7e95aff77cb8d438: name=etcd0 peerURLs=https://10.0.0.1:2380 clientURLs=https://10.0.0.1:2379 isLeader=true
b3464d77a82805dd: name=etcd1 peerURLs=https://10.0.0.2:2380 clientURLs=https://10.0.0.2:2379 isLeader=false
7c398b06160ff5ea: name=etcd2 peerURLs=https://10.0.0.3:2380 clientURLs=https://10.0.0.3:2379 isLeader=false
```
直接连接 etcd 的应用只有 apiserver 和 calico-node

在每个 master 上面都设置 haproxy:
/etc/haproxy/haproxy.conf:
```
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	user haproxy
	group haproxy
	daemon
	maxconn 7000

	stats socket /var/run/haproxy.sock mode 660 level admin
	stats timeout 2m # Wait up to 2 minutes for input

defaults
    mode tcp
    option tcplog
    option log-health-checks
    log global
    balance roundrobin
    timeout client      10m
    timeout server      10m
    timeout connect     10s
    timeout client-fin 5s
    timeout server-fin 5s
    timeout queue 5s
    retries 3

frontend masterapiserverdeployfrontend
    bind 10.0.0.1:443  (master1 上面是10.0.0.2:443 master2 上面是10.0.0.3:443)
    mode tcp
    log global
    option tcplog
    default_backend masterapiserverbackend

frontend masterapiserverfrontend
    bind 172.20.0.1:8443
    mode tcp
    log global
    option tcplog
    default_backend masterapiserverbackend

backend masterapiserverbackend
    mode tcp
    balance roundrobin
    log global
    option ssl-hello-chk
    option log-health-checks
    default-server inter 60s fall 3 rise 3
    server master0 10.0.0.1:8443 check
    server master1 10.0.0.2:8443 check
    server master2 10.0.0.3:8443 check

frontend masteretcdfrontend
    bind 172.20.0.1:4001
    mode tcp
    log global
    option tcplog
    default_backend masteretcdbackend

backend masteretcdbackend
    mode tcp
    balance roundrobin
    log global
    option ssl-hello-chk
    option log-health-checks
    default-server inter 60s  fall 3 rise 3
    server master0 10.0.0.1:2379 check
    server master1 10.0.0.2:2379 check
    server master2 10.0.0.3:2379 check
```    

在每个节点上面都安装 apiserver / schdule / controllermanager:
apiserver 配置为:
```
apiserver --bind-address=10.0.0.1 --insecure-bind-address=127.0.0.1 \
          --max-requests-inflight=0 --max-mutating-requests-inflight=0 \
          --storage-backend=etcd3 --etcd-servers=https://127.0.0.1:2379 \
          --allow-privileged=true --service-cluster-ip-range=172.20.0.0/16 \
          --secure-port=8443 --insecure-port=0 --cloud-provider=ibm \
          --cloud-config=/etc/kubernetes/ibm-cloud-config.ini \
          --advertise-address=172.20.0.1 --tls-cert-file=/etc/kubernetes/cert/apiserver.pem \
          --tls-private-key-file=/etc/kubernetes/cert/apiserver-key.pem \
          --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 \
          --client-ca-file=/etc/kubernetes/cert/ca.pem --etcd-cafile=/etc/kubernetes/cert/ca.pem \
          --etcd-certfile=/etc/kubernetes/cert/apiserver.pem --etcd-keyfile=/etc/kubernetes/cert/apiserver-key.pem \
          --authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1alpha1=true,batch/v2alpha1=true,admissionregistration.k8s.io/v1beta1=true,admissionregistration.k8s.io/v1alpha1=true \
          --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,Initializers,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,DefaultTolerationSeconds,StorageObjectInUseProtection \
          --proxy-client-cert-file=/etc/kubernetes/cert/apiserver.pem --proxy-client-key-file=/etc/kubernetes/cert/apiserver-key.pem \
          --service-node-port-range=20000-32767 --kubelet-client-certificate=/etc/kubernetes/cert/apiserver.pem \
          --kubelet-client-key=/etc/kubernetes/cert/apiserver-key.pem \
          --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true --feature-gates=TaintBasedEvictions=true --v=2
```
在 apiserver 的设置里面, 指定了 `etcd-endpoint`为`https://127.0.0.1:2379`, 意思是指当前的 apiserver 只访问自己节点上面的 etcd instance, 这样避免了远端的 etcd 调用, 降低网络压力, 并能降低响应时间. response time

controller-manager 的配置为:
```
controller-manager --use-service-account-credentials --kubeconfig=/etc/kubernetes/controller-manager-kubeconfig \
                   --service-account-private-key-file=/etc/kubernetes/cert/apiserver-key.pem \
                   --root-ca-file=/etc/kubernetes/cert/ca.pem --leader-elect=True \
                   --cloud-provider=ibm --cloud-config=/etc/kubernetes/ibm-cloud-config.ini \
                   --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true \
                   --feature-gates=TaintBasedEvictions=true --v=2
```

/etc/kubernetes/controller-manager-kubeconfig
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: controller-manager
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/cert/ca.pem
    apiVersion: v1
    server: https://172.20.0.1:8443
users:
- name: controller-manager
  user:
    client-certificate: /etc/kubernetes/cert/controller-manager.pem
    client-key: /etc/kubernetes/cert/controller-manager-key.pem
```

在controller-manange 的配置文件, 指定了 apiserver 的访问 ip 为`https://172.20.0.1:8443`, 这个 ip 是 haproxy 提供的一个 vip, 通过 haproxy 的分发, 用轮询的方式将对 apiserver 的调动分发到每个 nodes

schedule的配置为:
```
scheduler --kubeconfig=/etc/kubernetes/scheduler-kubeconfig --master=https://172.20.0.1:8443 \
          --leader-elect=True --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true \
          --feature-gates=TaintBasedEvictions=true --v=2 --policy-configmap=scheduler-policy-configmap \
          --policy-configmap-namespace=kube-system
```

scheduler-kubeconfig 的配置为:
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: scheduler
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/cert/ca.pem
    apiVersion: v1
    server: https://172.20.0.1:8443
users:
- name: scheduler
  user:
    client-certificate: /etc/kubernetes/cert/scheduler.pem
    client-key: /etc/kubernetes/cert/scheduler-key.pem
```
这里和 controller-manager 一样将 apiserver 指向到了 haproxy 的 vip

kube-proxy 的配置:
```
proxy --master=https://172.20.0.1:8443 --kubeconfig=/etc/kubernetes/kubelet-kubeconfig --cluster-cidr 172.16.0.0/16 \
      --iptables-sync-period 300s --iptables-min-sync-period 60s \
      --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true --hostname-override 10.0.0.1 --v=2
```
由于在每个 node 上面都配置了 haproxy, 这里 kube-proxy 上面的 master 指向 haproxy 的 vip/

kubelet 的配置为:
```
kubelet --resolv-conf /run/systemd/resolve/resolv.conf --allow-privileged=true --cloud-provider=ibm \
        --cloud-config=/etc/kubernetes/config.ini --cluster-dns=172.20.0.10 `(kube-dns service ip)` --cluster-domain=cluster.local \
        --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true \
        --pod-manifest-path=/etc/kubernetes/manifests --kubeconfig=/etc/kubernetes/kubelet-kubeconfig \
        --max-pods=80 --v=2 --kube-reserved-cgroup=/podruntime --kube-reserved=memory=4Gi \
        --system-reserved-cgroup=/system.slice --system-reserved=memory=6Gi --kubelet-cgroups=/podruntime/kubelet \
        --runtime-cgroups=/podruntime/runtime --enforce-node-allocatable=pods,kube-reserved \
        --kube-api-qps=20 --kube-api-burst=40 \
        --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 \
        --file-check-frequency=5s --eviction-hard=memory.available<2560Mi --hostname-override=10.0.0.1 \
        --pod-infra-container-image=pause:3.1 --anonymous-auth=false \
        --client-ca-file=/etc/kubernetes/cert/ca.pem --read-only-port=0 --network-plugin=cni --cni-conf-dir=/etc/cni/net.d \
        --cni-bin-dir=/opt/cni/bin
```

/etc/kubernetes/config.ini
```
[global]
version = 1.1.0
[kubernetes]
config-file = /etc/kubernetes/kubelet-kubeconfig
config-file = /etc/kubernetes/controller-manager-kubeconfig
[load-balancer-deployment]
image = keepalived:143
application = keepalived
vlan-ip-config-map = vlan-ip-config
[provider]
providerID = 
internalIP =
externalIP =
```

/etc/kubernetes/kubelet-kubeconfig:
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: kubelet
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/cert/ca.pem
    apiVersion: v1
    server: https://172.20.0.1:8443
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/cert/kubelet.pem
    client-key: /etc/kubernetes/cert/kubelet-key.pem
```
在这里. kubelet 指向了 haproxy 的 vip


admin-kubeconfig 的配置为:
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: admin
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority-data: 
    ...
    apiVersion: v1
    server: https://172.20.0.1:8443
```   

以 daemonset 的模式安装calico-node, [install guide](https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/), 配置为: 
修改`etcd_endpoints`为`https://172.20.0.1:2379`
```
kind: ConfigMap
apiVersion: v1
data:
  calico_backend: bird
  calico_mtu_override: "1480"
  cni_network_config: |-
  ...
  etcd_endpoints: https://172.20.0.1:2379
  veth_mtu: "1480"
```
