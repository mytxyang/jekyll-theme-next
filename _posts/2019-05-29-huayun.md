# 架构题 1:
一套业务系统, 由数据库(mysql, redis 和 postgres), api 网关服务, 各类处理业务的微服务, 前端组成

## 任务一:
请基于容器设计一套业务系统


## 任务二:
不采用容器和虚拟机, 基于 pacemaker+corosync 和 haproxy 等组件设计一套业务系统


## 要求:
1.整个系统无单点故障, 出现故障在秒内完成切换
2.系统的所有业务做到负载均衡, 业务压测时压力平均分配
3.整个系统待机,硬件故障和任何一个服务出现问题, 系统不受影响
4.整个系统资源消耗尽可能少
5.系统安全尽可能的考虑



Used StatefileSet to deply mysql cluster

[build sql server cluster on k8s](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/)

[build redis cluster](https://github.com/sanderploegsma/redis-cluster)

[link 2](https://rancher.com/blog/2019/deploying-redis-cluster/)

[link 3](https://github.com/Tiroshan/kubernetes-redis-cluster)

postgres cluster:

[k8s site](https://kubernetes.io/blog/2017/02/postgresql-clusters-kubernetes-statefulsets/)

[repo](https://github.com/CrunchyData/crunchy-containers)

[operator](https://github.com/CrunchyData/postgres-operator)

[readme](https://medium.com/@spektom/postgresql-ha-cluster-in-kubernetes-ae0dbb86a3de)

[postgres docker](https://github.com/paunin/PostDock)

```
redis-sts.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-cluster
data:
  update-node.sh: |
    #!/bin/sh
    REDIS_NODES="/data/nodes.conf"
    sed -i -e "/myself/ s/[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/${POD_IP}/" ${REDIS_NODES}
    exec "$@"
  redis.conf: |+
    cluster-enabled yes
    cluster-require-full-coverage no
    cluster-node-timeout 15000
    cluster-config-file /data/nodes.conf
    cluster-migration-barrier 1
    appendonly yes
    protected-mode no
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  serviceName: redis-cluster
  replicas: 6
  selector:
    matchLabels:
      app: redis-cluster
  template:
    metadata:
      labels:
        app: redis-cluster
    spec:
      containers:
      - name: redis
        image: redis:5.0.1-alpine
        ports:
        - containerPort: 6379
          name: client
        - containerPort: 16379
          name: gossip
        command: ["/conf/update-node.sh", "redis-server", "/conf/redis.conf"]
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        volumeMounts:
        - name: conf
          mountPath: /conf
          readOnly: false
        - name: data
          mountPath: /data
          readOnly: false
      volumes:
      - name: conf
        configMap:
          name: redis-cluster
          defaultMode: 0755
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
redis-svc.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: redis-cluster
spec:
  type: ClusterIP
  ports:
  - port: 6379
    targetPort: 6379
    name: client
  - port: 16379
    targetPort: 16379
    name: gossip
  selector:
    app: redis-cluster
 
 ---
apiVersion: v1
kind: Service
metadata:
  name: hit-counter-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
    targetPort: 5000
  selector:
      app: myapp
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hit-counter-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: calinrus/api-redis-ha:1.0
        ports:
        - containerPort: 5000


create cluter replic:
kubectl exec -it redis-cluster-0 -- redis-trib create --replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 ')

kubectl exec -it redis-cluster-0 -- redis-cli --cluster create --cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 ')

 ```

/*************/
```
apiVersion: extensions/vabeta1
kind: Deployment
metedata:
  name: tomcat-demo
spec:
  replicas: 6
  selector:
    matchLabels:
      app: tomcat-demo
  template:
    metadata:
      lebels:
        app: tomcat-demo
    spec:
      containers:
      - name: tomcat-demo
        image: tomcat:latest
	ports:
	- containerPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-demo
  annotations:
    service.beta.kubernetes.io/alicoud-loadba;ancer-address-type: intranet
spec:
  ports:
  - port: 8080
    targetPort: 8080
    name: tomcat-demo
  selector:
    app: tomcat-demo
  type: LoadBalancer
```
  








# 架构题 2:
三节点的 k8s, master 和 worker 共享三节点(3 master + 3 worker), 当把其中一台(也正好是 etcd master 节点)直接断电后, k8s 需要一段时间来恢复工作, 这个收敛时间如何分布? 如何优化收敛时间?

检查收敛时间:
- kubectl 命令恢复工作的收敛时间, e.g 通过观察 kubectl get node
- service 接通 new pod (在断电时 schedule 到其他节点的 new pod)的收敛时间, 通过持续 curl url 观察.

方案一种, 如果两个 nginx proxy 都失效的情况下, 集群将不可用
为此设想了方案二, 把 haproxy 设计到了各个 master 上面, 这样的好处是即使两个 proxy 都失效的情况下, k8s 集群不受影响, 唯一受影响的是外部环境不能 access 集群, 不过这个方案大大提高了架构的复杂度, 也算各有利弊吧.

方案一中, 收敛时间由 max(etcd 恢复时间, nginx 切换时间) 决定
nginx 的切换时间由`proxy_connect_timeout`的配置实现

方案二中, 收敛时间由 max(etcd 恢复时间, haproxy 切换时间) 决定
etcd 恢复时间主要是`election timeout`, 也就是选举超时决定, 当 leader 节点失效后, 其他的剩余节点在选举超时后重新发起选举, 选举成功后, 新的 leader节点继续工作. 一般选举超时的设定为`10 * RTT` (round-trip time), 这个是和网络环境相关的, 可以用 ping 来进行简单的测试
haproxy 的切换时间

Note: 注意 ssl 证书
```
# mkdir api-ha && cd api-ha
# cat k8s-csr.json    
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.115.4",
    "192.168.115.5",
    "192.168.115.6",
    "10.254.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "FuZhou",
      "L": "FuZhou",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

# cfssl gencert -ca=/etc/ssl/etcd/ca.pem \
  -ca-key=/etc/ssl/etcd/ca-key.pem \
  -config=/etc/ssl/etcd/ca-config.json \
  -profile=kubernetes k8s-csr.json | cfssljson -bare kubernetes

# mv *.pem /etc/kubernetes/ssl/
```
/************************************************/

nginx:
```

upstream apiserver {
        server 10.0.0.1:8443 weight=1 max_fails=0  fail_timeout=600;
        server 10.0.0.2:8443 weight=1 max_fails=0  fail_timeout=600;
        server 10.0.0.3:8443 weight=1 max_fails=0  fail_timeout=600; 
}
upstream etcdcluster {
        server 10.0.0.1:2379 weight=1 max_fails=0  fail_timeout=600;
        server 10.0.0.1:2379 weight=1 max_fails=0  fail_timeout=600;
        server 10.0.0.1:2379 weight=1 max_fails=0  fail_timeout=600; 
}
server {
    listen       8443;                                                         
    server_name  10.0.0.200;                                               
    client_max_body_size 1024M;

    location / {
        proxy_pass http://apiserver/;
        proxy_set_header Host $host:$server_port;
	proxy_connect_timeout 10;
    }
}
server {
    listen       2379;                                                         
    server_name  10.0.0.200;                                               
    client_max_body_size 1024M;

    location / {
        proxy_pass http://etcdcluster/;
        proxy_set_header Host $host:$server_port;
	proxy_connect_timeout 10;
    }
}
```


3节点信息:
```
hostname: ip
master0   10.0.0.1
master1   10.0.0.2
master2   10.0.0.3
```

3节点etcd-cluster, 安装步骤参看[install guide](https://jimmysong.io/kubernetes-handbook/practice/etcd-cluster-installation.html)

- heartbeat interval: 心跳间隔, 默认为 100ms == 0.5-1.5 RTT
- election timeout RTT(round-trip time): 选举超时, 默认为 1000ms = 10 RTT, 最大为 50000ms


1. master0:
```
/opt/bin/etcd --name etcd0 --data-dir /var/etcd/data --listen-client-urls https://10.0.0.1:2379,https://127.0.0.1:2379 \
              --listen-peer-urls https://10.0.0.1:2380 --advertise-client-urls https://10.0.0.1:4001 \
              --initial-advertise-peer-urls https://10.0.0.1:2380 --initial-cluster-token etcd-cluster-1 \
              --initial-cluster etcd0=https://10.0.0.1:2380,etcd1=https://10.0.0.2:2380,etcd2=https://10.0.0.3:2380 \
              --initial-cluster-state existing --auto-compaction-retention=1 --client-cert-auth \
              --trusted-ca-file=/etc/kubernetes/cert/ca.pem --cert-file=/etc/kubernetes/cert/etcd.pem \
              --key-file=/etc/kubernetes/cert/etcd-key.pem --peer-client-cert-auth \
              --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem --peer-cert-file=/etc/kubernetes/cert/etcd.pem \
              --peer-key-file=/etc/kubernetes/cert/etcd-key.pem --quota-backend-bytes=8589934592 = $((8*1024*1024*1024))
```

2. master1:
```
/opt/bin/etcd --name etcd1 --data-dir /var/etcd/data --listen-client-urls https://10.0.0.2:2379,https://127.0.0.1:2379 \
              --listen-peer-urls https://10.0.0.2:2380 --advertise-client-urls https://10.0.0.2:4001 \
              --initial-advertise-peer-urls https://10.0.0.2:2380 --initial-cluster-token etcd-cluster-1 \
              --initial-cluster etcd0=https://10.0.0.1:2380,etcd1=https://10.0.0.2:2380,etcd2=https://10.0.0.3:2380 \
              --initial-cluster-state existing --auto-compaction-retention=1 --client-cert-auth \
              --trusted-ca-file=/etc/kubernetes/cert/ca.pem --cert-file=/etc/kubernetes/cert/etcd.pem \
              --key-file=/etc/kubernetes/cert/etcd-key.pem --peer-client-cert-auth \
              --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem --peer-cert-file=/etc/kubernetes/cert/etcd.pem \
              --peer-key-file=/etc/kubernetes/cert/etcd-key.pem --quota-backend-bytes=8589934592
```

3. master2:
```
/opt/bin/etcd --name etcd2 --data-dir /var/etcd/data --listen-client-urls https://10.0.0.3:2379,https://127.0.0.1:2379 \
              --listen-peer-urls https://10.0.0.3:2380 --advertise-client-urls https://10.0.0.3:4001 \
              --initial-advertise-peer-urls https://10.0.0.3:2380 --initial-cluster-token etcd-cluster-1 \
              --initial-cluster etcd0=https://10.0.0.1:2380,etcd1=https://10.0.0.2:2380,etcd2=https://10.0.0.3:2380 \
              --initial-cluster-state existing --auto-compaction-retention=1 --client-cert-auth \
              --trusted-ca-file=/etc/kubernetes/cert/ca.pem --cert-file=/etc/kubernetes/cert/etcd.pem \
              --key-file=/etc/kubernetes/cert/etcd-key.pem --peer-client-cert-auth \
              --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem --peer-cert-file=/etc/kubernetes/cert/etcd.pem \
              --peer-key-file=/etc/kubernetes/cert/etcd-key.pem --quota-backend-bytes=8589934592
```

check etcd status:
```
etcdctl --endpoints=https://10.0.0.1:2379 --cert-file=/etc/kubernetes/cert/etcd.pem \
        --key-file=/etc/kubernetes/cert/etcd-key.pem --ca-file=/etc/kubernetes/cert/ca.pem member list
7e95aff77cb8d438: name=etcd0 peerURLs=https://10.0.0.1:2380 clientURLs=https://10.0.0.1:2379 isLeader=true
b3464d77a82805dd: name=etcd1 peerURLs=https://10.0.0.2:2380 clientURLs=https://10.0.0.2:2379 isLeader=false
7c398b06160ff5ea: name=etcd2 peerURLs=https://10.0.0.3:2380 clientURLs=https://10.0.0.3:2379 isLeader=false
```
直接连接 etcd 的应用只有 apiserver 和 calico-node

在每个 master 上面都设置 haproxy:
/etc/haproxy/haproxy.conf:
```
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	user haproxy
	group haproxy
	daemon
	maxconn 7000

	stats socket /var/run/haproxy.sock mode 660 level admin
	stats timeout 2m # Wait up to 2 minutes for input

defaults
    mode tcp
    option tcplog
    option log-health-checks
    log global
    balance roundrobin
    timeout client      10m
    timeout server      10m
    timeout connect     10s
    timeout client-fin 5s
    timeout server-fin 5s
    timeout queue 5s
    retries 3

frontend masterapiserverdeployfrontend
    bind 10.0.0.1:443  (master1 上面是10.0.0.2:443 master2 上面是10.0.0.3:443)
    mode tcp
    log global
    option tcplog
    default_backend masterapiserverbackend

frontend masterapiserverfrontend
    bind 172.20.0.1:8443
    mode tcp
    log global
    option tcplog
    default_backend masterapiserverbackend

backend masterapiserverbackend
    mode tcp
    balance roundrobin
    log global
    option ssl-hello-chk
    option log-health-checks
    default-server inter 60s fall 3 rise 3
    server master0 10.0.0.1:8443 check
    server master1 10.0.0.2:8443 check
    server master2 10.0.0.3:8443 check

frontend masteretcdfrontend
    bind 172.20.0.1:4001
    mode tcp
    log global
    option tcplog
    default_backend masteretcdbackend

backend masteretcdbackend
    mode tcp
    balance roundrobin
    log global
    option ssl-hello-chk
    option log-health-checks
    default-server inter 60s  fall 3 rise 3
    server master0 10.0.0.1:2379 check
    server master1 10.0.0.2:2379 check
    server master2 10.0.0.3:2379 check
```    

在每个节点上面都安装 apiserver / schdule / controllermanager:
apiserver 配置为:
```
apiserver --bind-address=10.0.0.1 --insecure-bind-address=127.0.0.1 \
          --max-requests-inflight=0 --max-mutating-requests-inflight=0 \
          --storage-backend=etcd3 --etcd-servers=https://127.0.0.1:2379 \
          --allow-privileged=true --service-cluster-ip-range=172.20.0.0/16 \
          --secure-port=8443 --insecure-port=0 --cloud-provider=ibm \
          --cloud-config=/etc/kubernetes/ibm-cloud-config.ini \
          --advertise-address=172.20.0.1 --tls-cert-file=/etc/kubernetes/cert/apiserver.pem \
          --tls-private-key-file=/etc/kubernetes/cert/apiserver-key.pem \
          --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 \
          --client-ca-file=/etc/kubernetes/cert/ca.pem --etcd-cafile=/etc/kubernetes/cert/ca.pem \
          --etcd-certfile=/etc/kubernetes/cert/apiserver.pem --etcd-keyfile=/etc/kubernetes/cert/apiserver-key.pem \
          --authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1alpha1=true,batch/v2alpha1=true,admissionregistration.k8s.io/v1beta1=true,admissionregistration.k8s.io/v1alpha1=true \
          --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,Initializers,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,DefaultTolerationSeconds,StorageObjectInUseProtection \
          --proxy-client-cert-file=/etc/kubernetes/cert/apiserver.pem --proxy-client-key-file=/etc/kubernetes/cert/apiserver-key.pem \
          --service-node-port-range=20000-32767 --kubelet-client-certificate=/etc/kubernetes/cert/apiserver.pem \
          --kubelet-client-key=/etc/kubernetes/cert/apiserver-key.pem \
          --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true --feature-gates=TaintBasedEvictions=true --v=2
```
在 apiserver 的设置里面, 指定了 `etcd-endpoint`为`https://127.0.0.1:2379`, 意思是指当前的 apiserver 只访问自己节点上面的 etcd instance, 这样避免了远端的 etcd 调用, 降低网络压力, 并能降低响应时间. response time

controller-manager 的配置为:
```
controller-manager --use-service-account-credentials --kubeconfig=/etc/kubernetes/controller-manager-kubeconfig \
                   --service-account-private-key-file=/etc/kubernetes/cert/apiserver-key.pem \
                   --root-ca-file=/etc/kubernetes/cert/ca.pem --leader-elect=True \
                   --cloud-provider=ibm --cloud-config=/etc/kubernetes/ibm-cloud-config.ini \
                   --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true \
                   --feature-gates=TaintBasedEvictions=true --v=2
```

/etc/kubernetes/controller-manager-kubeconfig
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: controller-manager
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/cert/ca.pem
    apiVersion: v1
    server: https://172.20.0.1:8443
users:
- name: controller-manager
  user:
    client-certificate: /etc/kubernetes/cert/controller-manager.pem
    client-key: /etc/kubernetes/cert/controller-manager-key.pem
```

在controller-manange 的配置文件, 指定了 apiserver 的访问 ip 为`https://172.20.0.1:8443`, 这个 ip 是 haproxy 提供的一个 vip, 通过 haproxy 的分发, 用轮询的方式将对 apiserver 的调动分发到每个 nodes

schedule的配置为:
```
scheduler --kubeconfig=/etc/kubernetes/scheduler-kubeconfig --master=https://172.20.0.1:8443 \
          --leader-elect=True --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true \
          --feature-gates=TaintBasedEvictions=true --v=2 --policy-configmap=scheduler-policy-configmap \
          --policy-configmap-namespace=kube-system
```

scheduler-kubeconfig 的配置为:
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: scheduler
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/cert/ca.pem
    apiVersion: v1
    server: https://172.20.0.1:8443
users:
- name: scheduler
  user:
    client-certificate: /etc/kubernetes/cert/scheduler.pem
    client-key: /etc/kubernetes/cert/scheduler-key.pem
```
这里和 controller-manager 一样将 apiserver 指向到了 haproxy 的 vip

kube-proxy 的配置:
```
proxy --master=https://172.20.0.1:8443 --kubeconfig=/etc/kubernetes/kubelet-kubeconfig --cluster-cidr 172.16.0.0/16 \
      --iptables-sync-period 300s --iptables-min-sync-period 60s \
      --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true --hostname-override 10.0.0.1 --v=2
```
由于在每个 node 上面都配置了 haproxy, 这里 kube-proxy 上面的 master 指向 haproxy 的 vip/

kubelet 的配置为:
```
kubelet --resolv-conf /run/systemd/resolve/resolv.conf --allow-privileged=true --cloud-provider=ibm \
        --cloud-config=/etc/kubernetes/config.ini --cluster-dns=172.20.0.10 `(kube-dns service ip)` --cluster-domain=cluster.local \
        --feature-gates=DevicePlugins=true,ExperimentalCriticalPodAnnotation=true \
        --pod-manifest-path=/etc/kubernetes/manifests --kubeconfig=/etc/kubernetes/kubelet-kubeconfig \
        --max-pods=80 --v=2 --kube-reserved-cgroup=/podruntime --kube-reserved=memory=4Gi \
        --system-reserved-cgroup=/system.slice --system-reserved=memory=6Gi --kubelet-cgroups=/podruntime/kubelet \
        --runtime-cgroups=/podruntime/runtime --enforce-node-allocatable=pods,kube-reserved \
        --kube-api-qps=20 --kube-api-burst=40 \
        --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 \
        --file-check-frequency=5s --eviction-hard=memory.available<2560Mi --hostname-override=10.0.0.1 \
        --pod-infra-container-image=pause:3.1 --anonymous-auth=false \
        --client-ca-file=/etc/kubernetes/cert/ca.pem --read-only-port=0 --network-plugin=cni --cni-conf-dir=/etc/cni/net.d \
        --cni-bin-dir=/opt/cni/bin
```

/etc/kubernetes/config.ini
```
[global]
version = 1.1.0
[kubernetes]
config-file = /etc/kubernetes/kubelet-kubeconfig
config-file = /etc/kubernetes/controller-manager-kubeconfig
[load-balancer-deployment]
image = keepalived:143
application = keepalived
vlan-ip-config-map = vlan-ip-config
[provider]
providerID = 
internalIP =
externalIP =
```

/etc/kubernetes/kubelet-kubeconfig:
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: kubelet
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/cert/ca.pem
    apiVersion: v1
    server: https://172.20.0.1:8443
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/cert/kubelet.pem
    client-key: /etc/kubernetes/cert/kubelet-key.pem
```
在这里. kubelet 指向了 haproxy 的 vip


admin-kubeconfig 的配置为:
```
current-context: service-account-context
apiVersion: v1
kind: Config
contexts:
- context:
    cluster: local
    user: admin
    namespace: default
  name: service-account-context
clusters:
- name: local
  cluster:
    certificate-authority-data: 
    ...
    apiVersion: v1
    server: https://172.20.0.1:8443
```   

以 daemonset 的模式安装calico-node, [install guide](https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/), 配置为: 
修改`etcd_endpoints`为`https://172.20.0.1:2379`
```
kind: ConfigMap
apiVersion: v1
data:
  calico_backend: bird
  calico_mtu_override: "1480"
  cni_network_config: |-
  ...
  etcd_endpoints: https://172.20.0.1:2379
  veth_mtu: "1480"
```


keepalived:
```
! Configuration File for keepalived

global_defs {
  script_user root
}

vrrp_sync_group G1 {
  group {
    VI_1
    VP_1
    VP_GATEWAY
    VI_GATEWAY
  }
  notify_master "/etc/conntrackd/primary-backup.sh primary"
  notify_backup "/etc/conntrackd/primary-backup.sh backup"
  notify_fault "/etc/conntrackd/primary-backup.sh fault"
}


vrrp_instance VI_1 {
    state BACKUP
    nopreempt
    interface bond1
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass bd8559dd
    }
    virtual_ipaddress {
        130.198.66.34 dev keepalived0
    }
}

vrrp_instance VP_1 {
    state BACKUP
    nopreempt
    interface bond0
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 17d8ce42
    }
    virtual_ipaddress {
        10.63.24.194 dev keepalived0
    }
}
vrrp_instance VP_GATEWAY {
    state BACKUP
    nopreempt
    interface bond1
    virtual_router_id 53
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 6157bc6e
    }
    virtual_ipaddress {
        130.198.66.36 dev keepalived0
    }
}

vrrp_instance VI_GATEWAY {
    state BACKUP
    nopreempt
    interface bond1
    virtual_router_id 52
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 748327dd
    }
    virtual_ipaddress {
        130.198.66.35 dev keepalived0
    }
}
virtual_server 130.198.66.34 0 {
    lb_algo rr
    lb_kind NAT
    persistence_timeout 5
    persistence_granularity 255.255.255.255
    protocol TCP
    real_server 130.198.74.152 0 {
        weight 1
        HTTP_GET {
            url {
                path /
                status_code 200
            }
            connect_timeout 3
            connect_port 20000
        }
    }
```

```
# cat /etc/conntrackd/primary-backup.sh
#!/bin/sh
#
# (C) 2006-2011 by Pablo Neira Ayuso <pablo@netfilter.org>
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# Description:
#
# This is the script for primary-backup setups for keepalived
# (http://www.keepalived.org). You may adapt it to make it work with other
# high-availability managers.
#
# Do not forget to include the required modifications to your keepalived.conf
# file to invoke this script during keepalived's state transitions.
#
# Contributions to improve this script are welcome :).
#

CONNTRACKD_BIN=/usr/sbin/conntrackd
CONNTRACKD_LOCK=/var/lock/conntrack.lock
CONNTRACKD_CONFIG=/etc/conntrackd/conntrackd.conf

case "$1" in
  primary)
    #
    # commit the external cache into the kernel table
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -c
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -c"
    fi

    #
    # flush the internal and the external caches
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -f
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -f"
    fi

    #
    # resynchronize my internal cache to the kernel table
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -R
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -R"
    fi

    #
    # send a bulk update to backups
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -B
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -B"
    fi
    ;;
  backup)
    #
    # is conntrackd running? request some statistics to check it
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -s
    if [ $? -eq 1 ]
    then
        #
    # something's wrong, do we have a lock file?
    #
        if [ -f $CONNTRACKD_LOCK ]
    then
        logger "WARNING: conntrackd was not cleanly stopped."
        logger "If you suspect that it has crashed:"
        logger "1) Enable coredumps"
        logger "2) Try to reproduce the problem"
        logger "3) Post the coredump to netfilter-devel@vger.kernel.org"
        rm -f $CONNTRACKD_LOCK
    fi
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -d
    if [ $? -eq 1 ]
    then
        logger "ERROR: cannot launch conntrackd"
        exit 1
    fi
    fi
    #
    # shorten kernel conntrack timers to remove the zombie entries.
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -t
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -t"
    fi

    #
    # request resynchronization with master firewall replica (if any)
    # Note: this does nothing in the alarm approach.
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -n
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -n"
    fi
    ;;
  fault)
    #
    # shorten kernel conntrack timers to remove the zombie entries.
    #
    $CONNTRACKD_BIN -C $CONNTRACKD_CONFIG -t
    if [ $? -eq 1 ]
    then
        logger "ERROR: failed to invoke conntrackd -t"
    fi
    ;;
  *)
    logger "ERROR: unknown state transition"
    echo "Usage: primary-backup.sh {primary|backup|fault}"
    exit 1
    ;;
esac

exit 0
```

测试:
1. 

2. 启动一个服务, 3 节点的 service:
```
apiVersion: apps/v1
kind: Deployment
metadata:
    name: test-deployment
spec:
    replicas: 3
    selector:
        matchLabels:
            app: test-pod
    template:
        metadata:
            labels:
                app: test-pod
        spec:
            containers:
            - name: python-http-server
              image: python:2.7
              command: ["/bin/bash"]
              args: ["-c", "echo \"Hello from $(hostname)\" > index.html; python -m SimpleHTTPServer 80"]
              ports:
              - name: http
                containerPort: 80

kind: Service
apiVersion: v1
metadata:
    name: test-service
spec:
    selector:
        app: test-pod
    ports:
    - protocol: TCP
      port: 4000
      targetPort: http
```

```
# kubectl get po
test-deployment-86878588dd-2zwph   1/1     Running            0          
test-deployment-86878588dd-bzxnb   1/1     Running            0         
test-deployment-86878588dd-fbj8z   1/1     Running            0         
#kubectl get svc
test-service   ClusterIP      172.21.217.128   <none>        4000/TCP
```

启动一个`kubectl proxy &`:
for i in `seq 1 1000`; do echo $i; curl http://127.0.0.1:8001/api/v1/namespaces/default/services/test-service:4000/proxy/; sleep 3; done
